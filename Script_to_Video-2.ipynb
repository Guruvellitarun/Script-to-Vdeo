{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ddc54be120c240abbf39346800631d02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d4f26da96f914eeab6372e1060d9baa7",
              "IPY_MODEL_0ff5f6b2463a45bbb804e11bcb2d040d",
              "IPY_MODEL_55ab512f50d649f7baca5dcd4e328286"
            ],
            "layout": "IPY_MODEL_dd09a23a5d804a96a574030e94c5e872"
          }
        },
        "d4f26da96f914eeab6372e1060d9baa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4b3ca3c771b4056a3ab95a0c9ecfd90",
            "placeholder": "​",
            "style": "IPY_MODEL_0202340789e146d1b523696fed258976",
            "value": "Loading pipeline components...: 100%"
          }
        },
        "0ff5f6b2463a45bbb804e11bcb2d040d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ef885d64c544e62830528018d95bd01",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d93bfcd0d41c47c18058a12b562744c1",
            "value": 5
          }
        },
        "55ab512f50d649f7baca5dcd4e328286": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aea481676de846b9b126df65fcf5e16b",
            "placeholder": "​",
            "style": "IPY_MODEL_5b046c9269344f2c8f0ba5d09cd3846e",
            "value": " 5/5 [00:00&lt;00:00,  4.91it/s]"
          }
        },
        "dd09a23a5d804a96a574030e94c5e872": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4b3ca3c771b4056a3ab95a0c9ecfd90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0202340789e146d1b523696fed258976": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ef885d64c544e62830528018d95bd01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d93bfcd0d41c47c18058a12b562744c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aea481676de846b9b126df65fcf5e16b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b046c9269344f2c8f0ba5d09cd3846e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8gxHl4KvRMBu",
        "outputId": "6a968de6-e6b7-4e9d-86b0-a25e7b192944"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Waiting for headers] [Connected to r2u.stat.illinois.edu (192\r                                                                                                    \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AXETkBXnk3UE"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio torch diffusers transformers numpy imageio imageio-ffmpeg gtts pydub moviepy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0G6aJDOYRPQ6",
        "outputId": "38e56736-d567-4ccd-b9ae-61f46c734ef2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.20.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (0.32.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (2.37.0)\n",
            "Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.11/dist-packages (0.6.0)\n",
            "Requirement already satisfied: gtts in /usr/local/lib/python3.11/dist-packages (2.5.4)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (0.25.1)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.11/dist-packages (1.0.3)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.11)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.7.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.7.2)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.5)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.9.9)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio) (14.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers) (8.6.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from diffusers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from diffusers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from diffusers) (0.5.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.11/dist-packages (from gtts) (8.1.8)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.11/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.11/dist-packages (from moviepy) (0.1.10)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (2.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers) (3.21.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\n",
        "import numpy as np\n",
        "import imageio\n",
        "import os\n",
        "from gtts import gTTS\n",
        "from moviepy.editor import VideoFileClip, AudioFileClip, concatenate_videoclips\n",
        "import tempfile\n",
        "import gradio as gr\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "# Set up your Hugging Face token\n",
        "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"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\"\n",
        "\n",
        "class TextToVideoGenerator:\n",
        "    def __init__(self):\n",
        "        # Load T2V pipeline with a stable model\n",
        "        print(\"Loading Text-to-Video model...\")\n",
        "        self.pipe = DiffusionPipeline.from_pretrained(\n",
        "            \"damo-vilab/text-to-video-ms-1.7b\",\n",
        "            torch_dtype=torch.float16,\n",
        "            variant=\"fp16\"\n",
        "        )\n",
        "\n",
        "        # Move to GPU if available\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        if self.device == \"cuda\":\n",
        "            self.pipe.to(self.device)\n",
        "            # Optimize for speed with less memory usage\n",
        "            self.pipe.scheduler = DPMSolverMultistepScheduler.from_config(self.pipe.scheduler.config)\n",
        "            self.pipe.enable_vae_slicing()\n",
        "        print(f\"Model loaded and running on {self.device}\")\n",
        "\n",
        "        # Initialize TTS pipeline\n",
        "        print(\"Loading Text-to-Speech engine...\")\n",
        "        self.tts = gTTS\n",
        "\n",
        "        # Store the last frame of previous segment for continuity\n",
        "        self.last_frame = None\n",
        "        self.style_seed = None\n",
        "\n",
        "        # Define voice configurations\n",
        "        self.voice_configs = {\n",
        "            \"Adult Male\": {\"lang\": \"en\", \"tld\": \"com\"},\n",
        "            \"Adult Female\": {\"lang\": \"en\", \"tld\": \"co.uk\"},\n",
        "            \"Child\": {\"lang\": \"en\", \"tld\": \"com.au\"},\n",
        "            \"Elderly\": {\"lang\": \"en\", \"tld\": \"ca\"},\n",
        "            \"Professional\": {\"lang\": \"en\", \"tld\": \"ie\"}\n",
        "        }\n",
        "\n",
        "    def generate_segment(self, prompt, num_frames=16, fps=8, guidance_scale=9.0,\n",
        "                         seed=None, continuation_strength=0.3,\n",
        "                         progress=gr.Progress(), progress_start=0, progress_end=0.33):\n",
        "        \"\"\"Generate a single video segment from a text prompt with continuity from previous segment\"\"\"\n",
        "        progress(progress_start, desc=f\"Generating video segment for: {prompt[:30]}...\")\n",
        "\n",
        "        # Set seed for consistency\n",
        "        generator = None\n",
        "        if seed is not None:\n",
        "            generator = torch.Generator(device=self.device).manual_seed(seed)\n",
        "\n",
        "        # Generate video frames\n",
        "        result = self.pipe(\n",
        "            prompt=prompt,\n",
        "            num_inference_steps=25,\n",
        "            num_frames=num_frames,\n",
        "            guidance_scale=guidance_scale,\n",
        "            generator=generator\n",
        "        )\n",
        "\n",
        "        progress((progress_start + progress_end) / 2, desc=\"Processing video segment...\")\n",
        "\n",
        "        # Extract frames correctly based on the model output format\n",
        "        if hasattr(result, 'frames'):\n",
        "            frames = result.frames\n",
        "            if isinstance(frames, list):\n",
        "                frames = np.array(frames)\n",
        "            elif isinstance(frames, np.ndarray) and frames.ndim == 5:\n",
        "                frames = frames[0]\n",
        "        elif hasattr(result, 'videos'):\n",
        "            videos = result.videos\n",
        "            if isinstance(videos, torch.Tensor):\n",
        "                videos = videos.cpu().numpy()\n",
        "            if videos.ndim == 5:\n",
        "                videos = videos[0]\n",
        "                if videos.shape[0] == 3:\n",
        "                    videos = np.transpose(videos, (1, 2, 3, 0))\n",
        "                frames = videos\n",
        "            else:\n",
        "                frames = videos\n",
        "        else:\n",
        "            raise ValueError(\"Unexpected model output format\")\n",
        "\n",
        "        # Make sure frames are in the right format\n",
        "        if frames.ndim == 5:\n",
        "            frames = frames[0]\n",
        "\n",
        "        # Convert float32 to uint8 for video saving if needed\n",
        "        if frames.dtype == np.float32 or frames.dtype == np.float64:\n",
        "            frames = (frames * 255).astype(np.uint8)\n",
        "\n",
        "        # Apply frame blending for continuity if this isn't the first segment\n",
        "        if self.last_frame is not None:\n",
        "            # Create a smooth transition by blending the first few frames with the last frame of previous segment\n",
        "            blend_frames = min(4, len(frames))\n",
        "            for i in range(blend_frames):\n",
        "                # Calculate blend ratio (gradually reduce influence of previous frame)\n",
        "                blend_ratio = continuation_strength * (1.0 - i / blend_frames)\n",
        "                # Blend current frame with last frame\n",
        "                frames[i] = cv2.addWeighted(\n",
        "                    self.last_frame, blend_ratio,\n",
        "                    frames[i], 1.0 - blend_ratio,\n",
        "                    0\n",
        "                )\n",
        "\n",
        "        # Save the last frame for next segment's continuity\n",
        "        self.last_frame = frames[-1].copy()\n",
        "\n",
        "        # Clear CUDA cache\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        progress(progress_end, desc=f\"Segment completed\")\n",
        "        return frames\n",
        "\n",
        "    def generate_video(self, prompt, num_segments=3, num_frames=16, fps=8, guidance_scale=9.0, progress=gr.Progress()):\n",
        "        \"\"\"Generate multiple continuous video segments and combine them\"\"\"\n",
        "        progress(0, desc=\"Starting multi-segment video generation...\")\n",
        "\n",
        "        # Create temporary directory for files\n",
        "        temp_dir = tempfile.mkdtemp()\n",
        "\n",
        "        # Reset continuity variables\n",
        "        self.last_frame = None\n",
        "\n",
        "        # Set a style seed for consistency in style across segments\n",
        "        self.style_seed = torch.randint(1, 100000, (1,)).item()\n",
        "\n",
        "        # Create prompts for each segment that maintain scene continuity\n",
        "        segment_prompts = []\n",
        "\n",
        "        # First segment uses the original prompt\n",
        "        segment_prompts.append(prompt)\n",
        "\n",
        "        # Create better prompts for continuity\n",
        "        transition_words = [\n",
        "            \"continuing, the same scene with\",\n",
        "            \"following directly, the same view of\",\n",
        "            \"next moment in the same scene,\"\n",
        "        ]\n",
        "\n",
        "        # Create modified prompts for continuation segments\n",
        "        for i in range(1, num_segments):\n",
        "            # Add different continuity modifiers\n",
        "            transition = transition_words[min(i-1, len(transition_words)-1)]\n",
        "            continuation_prompt = f\"{transition} {prompt}\"\n",
        "            segment_prompts.append(continuation_prompt)\n",
        "\n",
        "        # Generate each segment with seeds that create style consistency\n",
        "        all_frames = []\n",
        "        for i in range(num_segments):\n",
        "            # Calculate progress values for this segment\n",
        "            progress_start = i * (0.7 / num_segments)\n",
        "            progress_end = (i + 1) * (0.7 / num_segments)\n",
        "\n",
        "            # Generate segment using a seed derived from the style seed\n",
        "            # This maintains style consistency while allowing progression\n",
        "            segment_seed = self.style_seed + (i * 5)  # Small increments for controlled variation\n",
        "\n",
        "            frames = self.generate_segment(\n",
        "                segment_prompts[i],\n",
        "                num_frames=num_frames,\n",
        "                fps=fps,\n",
        "                guidance_scale=guidance_scale,\n",
        "                seed=segment_seed,\n",
        "                continuation_strength=0.3 if i > 0 else 0.0,  # Only apply continuity after first segment\n",
        "                progress=progress,\n",
        "                progress_start=progress_start,\n",
        "                progress_end=progress_end\n",
        "            )\n",
        "            all_frames.append(frames)\n",
        "\n",
        "        # Save individual segments temporarily\n",
        "        segment_paths = []\n",
        "        for i, frames in enumerate(all_frames):\n",
        "            segment_path = os.path.join(temp_dir, f\"segment_{i}.mp4\")\n",
        "            imageio.mimsave(segment_path, frames, fps=fps)\n",
        "            segment_paths.append(segment_path)\n",
        "\n",
        "        # Combine segments using moviepy for a smoother transition\n",
        "        progress(0.75, desc=\"Combining video segments...\")\n",
        "        clips = [VideoFileClip(path) for path in segment_paths]\n",
        "\n",
        "        # Use cross-fade transitions between clips\n",
        "        combined_clip = concatenate_videoclips(clips, method=\"compose\")\n",
        "\n",
        "        # Save combined video\n",
        "        video_path = os.path.join(temp_dir, \"generated_video.mp4\")\n",
        "        combined_clip.write_videofile(video_path, codec=\"libx264\", audio=False, fps=fps)\n",
        "\n",
        "        # Close clips to free resources\n",
        "        for clip in clips:\n",
        "            clip.close()\n",
        "\n",
        "        progress(0.8, desc=\"Combined video created\")\n",
        "        return video_path, temp_dir\n",
        "\n",
        "    def generate_audio(self, text, voice_type, temp_dir):\n",
        "        \"\"\"Generate audio from text using gTTS with specified voice type\"\"\"\n",
        "        audio_path = os.path.join(temp_dir, \"voiceover.mp3\")\n",
        "\n",
        "        # Get voice configuration\n",
        "        voice_config = self.voice_configs.get(voice_type, self.voice_configs[\"Adult Male\"])\n",
        "\n",
        "        # Generate audio with the selected voice configuration\n",
        "        self.tts(\n",
        "            text=text,\n",
        "            lang=voice_config[\"lang\"],\n",
        "            tld=voice_config[\"tld\"],\n",
        "            slow=voice_type == \"Child\"  # Slow down for child voice\n",
        "        ).save(audio_path)\n",
        "\n",
        "        return audio_path\n",
        "\n",
        "    def combine_video_audio(self, video_path, audio_path, progress=gr.Progress()):\n",
        "        \"\"\"Combine video and audio files\"\"\"\n",
        "        progress(0.9, desc=\"Combining video and audio...\")\n",
        "\n",
        "        output_path = video_path.replace(\".mp4\", \"_with_audio.mp4\")\n",
        "\n",
        "        # Load video and audio clips\n",
        "        video_clip = VideoFileClip(video_path)\n",
        "        audio_clip = AudioFileClip(audio_path)\n",
        "\n",
        "        # If audio is longer than video, extend video by looping\n",
        "        if audio_clip.duration > video_clip.duration:\n",
        "            # Calculate how many times to repeat the video\n",
        "            repeat_times = int(np.ceil(audio_clip.duration / video_clip.duration))\n",
        "            video_clips = [video_clip] * repeat_times\n",
        "            extended_video = concatenate_videoclips(video_clips)\n",
        "            # Trim the extended video to match audio duration\n",
        "            final_video = extended_video.subclip(0, audio_clip.duration)\n",
        "        else:\n",
        "            final_video = video_clip\n",
        "            # Trim audio if needed\n",
        "            audio_clip = audio_clip.subclip(0, min(video_clip.duration, audio_clip.duration))\n",
        "\n",
        "        # Set audio to video\n",
        "        final_video = final_video.set_audio(audio_clip)\n",
        "\n",
        "        # Write output file\n",
        "        final_video.write_videofile(output_path, codec=\"libx264\", audio_codec=\"aac\", fps=24)\n",
        "\n",
        "        # Close clips to free resources\n",
        "        video_clip.close()\n",
        "        audio_clip.close()\n",
        "\n",
        "        progress(1.0, desc=\"Video with voiceover created!\")\n",
        "        return output_path\n",
        "\n",
        "    def process(self, prompt, script, voice_type, progress=gr.Progress()):\n",
        "        \"\"\"Process the entire pipeline from text to video with audio\"\"\"\n",
        "        try:\n",
        "            progress(0.1, desc=\"Starting process...\")\n",
        "            # Generate segments for a longer video\n",
        "            video_path, temp_dir = self.generate_video(prompt, num_segments=3, progress=progress)\n",
        "\n",
        "            # Generate audio with selected voice type\n",
        "            progress(0.8, desc=f\"Generating {voice_type} voiceover...\")\n",
        "            audio_path = self.generate_audio(script, voice_type, temp_dir)\n",
        "\n",
        "            # Combine video and audio\n",
        "            final_path = self.combine_video_audio(video_path, audio_path, progress=progress)\n",
        "\n",
        "            return final_path\n",
        "        except Exception as e:\n",
        "            import traceback\n",
        "            error_details = traceback.format_exc()\n",
        "            print(f\"Error: {str(e)}\\n{error_details}\")\n",
        "            return f\"Error: {str(e)}\"\n",
        "\n",
        "# Create the Gradio interface\n",
        "def create_ui():\n",
        "    generator = TextToVideoGenerator()\n",
        "    with gr.Blocks(title=\"Script to Motion Video Generator\") as app:\n",
        "        with gr.Row():\n",
        "            # Left column for title and instructions\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\n",
        "                    \"\"\"\n",
        "                    # Automatic Script to Motion Video Generator\n",
        "                    Transform your written scripts into dynamic motion videos with AI-generated visuals and voiceovers.\n",
        "                    \"\"\"\n",
        "                )\n",
        "\n",
        "                gr.Markdown(\"## How to use:\")\n",
        "                gr.Markdown(\n",
        "                    \"\"\"\n",
        "                    1. Enter a visual prompt describing what you want to see in the video\n",
        "                    2. Enter your script text for the voiceover\n",
        "                    3. Select the voice type for your voiceover\n",
        "                    4. Click \"Generate\" and wait for the magic to happen!\n",
        "                    \"\"\"\n",
        "                )\n",
        "\n",
        "        with gr.Row(equal_height=True):\n",
        "            # Left column for inputs\n",
        "            with gr.Column(scale=1):\n",
        "                prompt_input = gr.Textbox(\n",
        "                    label=\"Visual Prompt\",\n",
        "                    placeholder=\"A beautiful sunset over mountains with clouds...\",\n",
        "                    lines=4,\n",
        "                    elem_id=\"prompt_box\"\n",
        "                )\n",
        "\n",
        "                with gr.Row():\n",
        "                    with gr.Column(scale=3):\n",
        "                        script_input = gr.Textbox(\n",
        "                            label=\"Voiceover Script\",\n",
        "                            placeholder=\"Enter the script text here that will be converted to speech...\",\n",
        "                            lines=8,\n",
        "                            elem_id=\"script_box\"\n",
        "                        )\n",
        "\n",
        "                    with gr.Column(scale=1):\n",
        "                        voice_dropdown = gr.Dropdown(\n",
        "                            choices=list(generator.voice_configs.keys()),\n",
        "                            value=\"Adult Male\",\n",
        "                            label=\"Voice Type\",\n",
        "                            elem_id=\"voice_dropdown\"\n",
        "                        )\n",
        "\n",
        "                generate_btn = gr.Button(\"Generate Video\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "            # Right column for video output\n",
        "            with gr.Column(scale=1):\n",
        "                # Video output with custom height to match input boxes\n",
        "                video_output = gr.Video(\n",
        "                    label=\"Generated Video\",\n",
        "                    height=470,  # Approximate height to match prompt + script boxes combined\n",
        "                    elem_id=\"video_display\"\n",
        "                )\n",
        "\n",
        "        with gr.Row():\n",
        "            # Tips section at the bottom\n",
        "            with gr.Column():\n",
        "                gr.Markdown(\n",
        "                    \"\"\"\n",
        "                    ### Tips for better results:\n",
        "                    - Provide detailed visual prompts for more precise video generation\n",
        "                    - Keep scripts concise and clear for better voiceover quality\n",
        "                    - Using descriptive prompts with specific themes will help maintain visual consistency\n",
        "                    - For best results, describe a continuous scene rather than multiple different scenes\n",
        "                    - Different voice types can enhance the mood of your video\n",
        "                    \"\"\"\n",
        "                )\n",
        "\n",
        "\n",
        "        generate_btn.click(\n",
        "            fn=generator.process,\n",
        "            inputs=[prompt_input, script_input, voice_dropdown],\n",
        "            outputs=video_output\n",
        "        )\n",
        "\n",
        "    return app\n",
        "\n",
        "# Run either the test or the UI\n",
        "if __name__ == \"__main__\":\n",
        "    import sys\n",
        "    if len(sys.argv) > 1 and sys.argv[1] == \"--test\":\n",
        "        # Test class\n",
        "        class MockProgress:\n",
        "            def __call__(self, value, desc=\"\"):\n",
        "                print(f\"Progress: {value*100:.0f}% - {desc}\")\n",
        "\n",
        "        generator = TextToVideoGenerator()\n",
        "        prompt = \"A beautiful sunset over mountains\"\n",
        "        script = \"This is a test of our text to video generation system with voiceover capabilities.\"\n",
        "        voice_type = \"Adult Male\"\n",
        "        result = generator.process(prompt, script, voice_type, MockProgress())\n",
        "        print(f\"Test completed successfully. Output video: {result}\")\n",
        "    else:\n",
        "        app = create_ui()\n",
        "        app.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 805,
          "referenced_widgets": [
            "ddc54be120c240abbf39346800631d02",
            "d4f26da96f914eeab6372e1060d9baa7",
            "0ff5f6b2463a45bbb804e11bcb2d040d",
            "55ab512f50d649f7baca5dcd4e328286",
            "dd09a23a5d804a96a574030e94c5e872",
            "d4b3ca3c771b4056a3ab95a0c9ecfd90",
            "0202340789e146d1b523696fed258976",
            "3ef885d64c544e62830528018d95bd01",
            "d93bfcd0d41c47c18058a12b562744c1",
            "aea481676de846b9b126df65fcf5e16b",
            "5b046c9269344f2c8f0ba5d09cd3846e"
          ]
        },
        "id": "KsjYiNbZUk_B",
        "outputId": "3d081c52-91ca-4917-ba73-ee12101868dd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Text-to-Video model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ddc54be120c240abbf39346800631d02"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded and running on cuda\n",
            "Loading Text-to-Speech engine...\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://99df299b6d13fbbfea.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://99df299b6d13fbbfea.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jkXyVafUdbJq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}